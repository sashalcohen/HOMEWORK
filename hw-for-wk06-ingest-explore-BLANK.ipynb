{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Qim5YnoxEywd",
   "metadata": {
    "id": "Qim5YnoxEywd"
   },
   "source": [
    "# Homework for Week 3\n",
    "\n",
    "The goal for this homework is to see the relevant journalistic uses for what we learned in class in week 2.\n",
    "\n",
    "1. access specific data within a dataset,\n",
    "2. break down larger datasets into smaller chunks (subset),\n",
    "3. Ingest different types of spreadsheets into Pandas,\n",
    "4. Gather general info about our dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J2Mf4b3sPvLg",
   "metadata": {
    "id": "J2Mf4b3sPvLg"
   },
   "source": [
    "# List Slicing\n",
    "## Natural disasters list\n",
    "\n",
    "The Small Business Administration has a list of natural disasters that cause extensive economic damage. Run the next cell to pull the list into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lk8GeCDvPvLg",
   "metadata": {
    "id": "lk8GeCDvPvLg"
   },
   "outputs": [],
   "source": [
    "disasters = [\n",
    "        \"Landslides\",\n",
    "        \"Earthquake\",\n",
    "        \"Wildfires\",\n",
    "        \"Tornadoes\",\n",
    "        \"Hurricane\",\n",
    "        \"Wind Storm\",\n",
    "        \"Winter Freeze\",\n",
    "        \"Ice Storm\",\n",
    "        \"Drought\",\n",
    "        \"Tsunami\",\n",
    "        \"Flash floods\",\n",
    "        \"Mudslides\",\n",
    "        \"Torrential rainfall\",\n",
    "        \"Riverine floods\",\n",
    "        \"Tidal Surge\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "AJE4jVfQPvLh",
   "metadata": {
    "id": "AJE4jVfQPvLh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Landslides',\n",
       " 'Earthquake',\n",
       " 'Wildfires',\n",
       " 'Tornadoes',\n",
       " 'Hurricane',\n",
       " 'Wind Storm',\n",
       " 'Winter Freeze',\n",
       " 'Ice Storm',\n",
       " 'Drought',\n",
       " 'Tsunami',\n",
       " 'Flash floods',\n",
       " 'Mudslides',\n",
       " 'Torrential rainfall',\n",
       " 'Riverine floods',\n",
       " 'Tidal Surge']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## call the disaster list here\n",
    "disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "EvzyETnYPvLh",
   "metadata": {
    "id": "EvzyETnYPvLh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landslides'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the first item in the list\n",
    "disasters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bOz8lgDCPvLh",
   "metadata": {
    "id": "bOz8lgDCPvLh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tidal Surge'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## call the penultimate item (without manually counting the number of items)\n",
    "disasters[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wWZaDxpPvLh",
   "metadata": {
    "id": "8wWZaDxpPvLh"
   },
   "source": [
    "### Clarification:\n",
    "\n",
    "For the next two questions, I am referring to numerical positions rather than zero-indexed positions. In other words, you will have to translate the numerical position into zero-indexed positions in your slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mN1kYkmcPvLh",
   "metadata": {
    "id": "mN1kYkmcPvLh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wildfires', 'Tornadoes', 'Hurricane', 'Wind Storm']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a new list called disasters_subset that holds the 3rd to 6th items inclusive\n",
    "disasters_subset = disasters[2:6]\n",
    "disasters_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "IyWaWlfuPvLh",
   "metadata": {
    "id": "IyWaWlfuPvLh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Earthquake',\n",
       " 'Wildfires',\n",
       " 'Tornadoes',\n",
       " 'Hurricane',\n",
       " 'Wind Storm',\n",
       " 'Winter Freeze',\n",
       " 'Ice Storm']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a new list called disasters_subset2 that holds the 2nd through 8th items\n",
    "disasters_subset2 = disasters[1:8]\n",
    "disasters_subset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64966a",
   "metadata": {
    "id": "1d64966a"
   },
   "source": [
    "## Pandas - Importing data\n",
    "\n",
    "Please download from Google Drive the following data and then upload it into this notebook:\n",
    "\n",
    "- <a href=\"https://drive.google.com/file/d/1izg0nXFm8vBJAhz77LFdSMQGP4STvpEk/view?usp=sharing\">EMS excerpt</a>,\n",
    "\n",
    "- <a href=\"https://docs.google.com/spreadsheets/d/1sUEttreon8IKzCmsfRQ6cPA1lfeuNDu9/edit?usp=sharing&ouid=107646296031550187966&rtpof=true&sd=true\">Customs, Border Patrol</a>\n",
    "\n",
    "**Give yourself time to download it. Some of the files are large!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c6fb4b",
   "metadata": {
    "id": "e8c6fb4b"
   },
   "outputs": [],
   "source": [
    "## import the pandas library in this cell\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d55dd",
   "metadata": {
    "id": "415d55dd"
   },
   "source": [
    "### Ingest the following dataset into this note:\n",
    "\n",
    "- ```ems20_1st_6months_excerpt.csv``` into a dataframe named ```ems2020_excerpt_df```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2609f6fb",
   "metadata": {
    "id": "2609f6fb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ems20_1st_6months_excerpt.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## import data and store in dataframe\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mems20_1st_6months_excerpt.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ems20_1st_6months_excerpt.csv'"
     ]
    }
   ],
   "source": [
    "## import data and store in dataframe\n",
    "\n",
    "pd.read_csv(\"ems20_1st_6months_excerpt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cb6b2",
   "metadata": {
    "id": "ec3cb6b2"
   },
   "outputs": [],
   "source": [
    "## call the first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59614c",
   "metadata": {
    "id": "fb59614c"
   },
   "outputs": [],
   "source": [
    "## call the last 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953ae2d",
   "metadata": {
    "id": "0953ae2d"
   },
   "outputs": [],
   "source": [
    "## call a 20 random rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Vyl0p7YHU_D",
   "metadata": {
    "id": "7Vyl0p7YHU_D"
   },
   "outputs": [],
   "source": [
    "## how might you slice this df to show only rows 42 through 51 inclusive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kUfizScRIMms",
   "metadata": {
    "id": "kUfizScRIMms"
   },
   "outputs": [],
   "source": [
    "## store rows 42 through 51 inclusive into a df called df_excerpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c744d9",
   "metadata": {
    "id": "68c744d9"
   },
   "outputs": [],
   "source": [
    "## get basic overview of the data\n",
    "## what datatypes do the different columns hold?\n",
    "## how many rows and columns are there\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f37584",
   "metadata": {
    "id": "e5f37584"
   },
   "source": [
    "## Targeting an Excel sheet\n",
    "\n",
    "We need the Customs and Border Protection data in this ```multi-data.xlsx``` workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1ee71",
   "metadata": {
    "id": "51a1ee71"
   },
   "outputs": [],
   "source": [
    "## import data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885c25d",
   "metadata": {
    "id": "8885c25d"
   },
   "outputs": [],
   "source": [
    "## call the top 15 rows\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
